<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>使用手记 on 后端小筑</title>
    <link>http://localhost:1313/categories/%E4%BD%BF%E7%94%A8%E6%89%8B%E8%AE%B0/</link>
    <description>Recent content in 使用手记 on 后端小筑</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 26 Sep 2017 23:29:00 +0800</lastBuildDate>
    <atom:link href="/categories/%E4%BD%BF%E7%94%A8%E6%89%8B%E8%AE%B0/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Go爬虫初探</title>
      <link>http://localhost:1313/post/2017/10/go%E7%88%AC%E8%99%AB%E5%88%9D%E6%8E%A2/</link>
      <pubDate>Tue, 26 Sep 2017 23:29:00 +0800</pubDate>
      
      <guid>http://localhost:1313/post/2017/10/go%E7%88%AC%E8%99%AB%E5%88%9D%E6%8E%A2/</guid>
      <description>背景 前些天用Python实践了一下爬虫功能，强大的requests库、re库、beautifulsoup库和scrapy框架以及其附属的分布式数据库配套库，开发起来十分高效。对于复杂的爬虫规则和验证规则，Python的相关生态也能提供很好的解决方案。回到golang，也想尝试是否能用go语言也构建一个基本爬虫，看实现上会有多大差异。
目标功能 获取 http://razil.cc 首页上的网站名称元素内容。
目标内容为“后端小筑”
基本结构 由于是基本的定向爬虫，类比Python的结构是
requests库 + re库
即基本请求后处理转码，再通过正则获得需要的数据
在go实现的结构如下:
 [net/http] 基本请求
 [mahonia] 第三方库实现编码转换
 [regexp] 正则匹配数据
  代码实现 package main import ( &amp;quot;errors&amp;quot; &amp;quot;fmt&amp;quot; &amp;quot;io/ioutil&amp;quot; &amp;quot;net/http&amp;quot; &amp;quot;regexp&amp;quot; encodingConvert &amp;quot;github.com/DennisMao/mahonia&amp;quot; ) func SimpleScrapy(url string) (string, error) { //http请求 resp, err := http.Get(url) if err != nil { return &amp;quot;&amp;quot;, err } //获取响应数据 if resp.StatusCode != 200 { return &amp;quot;&amp;quot;, errors.New(&amp;quot;resp statuscode = :&amp;quot; + fmt.</description>
    </item>
    
    <item>
      <title>Websocket使用手记</title>
      <link>http://localhost:1313/post/2017/07/websocket%E4%BD%BF%E7%94%A8%E6%89%8B%E8%AE%B0/</link>
      <pubDate>Thu, 21 Sep 2017 23:51:49 +0800</pubDate>
      
      <guid>http://localhost:1313/post/2017/07/websocket%E4%BD%BF%E7%94%A8%E6%89%8B%E8%AE%B0/</guid>
      <description> websocket是什么? Websocket最起初是在HTTP协议的基础上发展而来，后独立成为了一个标准。其特点是只要建立一次连接，即可保持服务端与客户端的长连接状态，期间可保持全双工的通讯(实时双方互传)。由于省去了重复建立连接产生的性能和带宽消耗，WebSocket可实现高性能的数据实时传递。
用于什么场景? 可应用在前后端数据需要实时数据互传的情况，比如：
 聊天系统 股票显示 实时对战游戏 你画我猜游戏  为什么要选用它? 在网页数据前后端交互上与WebSocket相近的解决方案是长连接，其原理通过HTTP协议建立一个tcp连接，期间交互通信仍然采用HTTP方式。而Websocket的区别就在于连接后的通信采用的是其独立的方式，对比起HTTP的请求头要精简许多，这意味着其交互的实时性优于长连接。 另一方面，目前websocket的兼容性还是需要提高。
所以在实际应用场景上，如果数据交互频率大，
使用过程 服务端 目前Go语言主流的Websocket库有两个：
+ 官方golang.org/x/websocket 库 + github.com/gorilla/websocket 库 官方的库目前支持情况不太理想,godoc官方的websocket包页面介绍上推荐使用Gorilla的库。两者对比下，官方的包目前能够实现基本的服务器和客户端连接、收发功能。如果需要快捷开发基本WebSocket功能可使用官方包。需要协议的高级功能的推荐采用Gorilla包。
Gorilla/websocket包，丰富了支持Websocket协议内的数据压缩、心跳检测事件、碎片信息。
程序结构
websocket &amp;mdash; main() &amp;ndash; 注册普通HTTP路由 &amp;ndash; 监听HTTP服务 &amp;mdash; websocket_handler() &amp;ndash; Upgrade 升级普通HTTP协议为Websocket协议，建立连接 &amp;ndash; For循环 &amp;ndash; ReadMessage 读取信息 &amp;ndash; WriteMessage 写入信息
服务端的编写并不复杂，与普通HTTP服务编写相近，先编写一个Handler处理函数，再在主程序中注册路由，开启服务，不同的主要在处理函数中。由于普通HTTP的响应都是无状态，即请求后响应即可退出协程，但Websocket需要协程一直在跑，而且句柄固定，因此我们需要在Websocket的处理函数中加入循环，不让它关闭。
 客户端主动断开连接 服务端接收信息失败 服务端发送信息失败 心跳检查失败  客户端 javascript golang </description>
    </item>
    
    <item>
      <title>Proto3使用手记</title>
      <link>http://localhost:1313/post/2017/08/proto3%E4%BD%BF%E7%94%A8%E6%89%8B%E8%AE%B0/</link>
      <pubDate>Tue, 22 Aug 2017 23:49:14 +0800</pubDate>
      
      <guid>http://localhost:1313/post/2017/08/proto3%E4%BD%BF%E7%94%A8%E6%89%8B%E8%AE%B0/</guid>
      <description>Proto3 是什么？ 最近正在使用gRPC，当前版本gRPC使用的是google开发的Proto 3版本，因此顺带写一篇。Proto 3全称是 Protocol buffers v3.0,是谷歌公司开发的一款序列化数据结构。支持多种语言。与proto2相比主要有以下几点改变：
 原生支持更多语言 JavaNano,Go,Ruby,Objective-C和C#等 调整部分语法 比如 default,optional,repeated 使得语法更简洁
 增加部分特性   具体改变介绍推荐以下这篇文章
Protobuf 的 proto3 与 proto2 的区别
最新特性详情可查阅
Protobuf Release
Proto3 有何功能? 通过定义接口描述文件 .proto文件，可通过这些描述自动生成出响应代码，用于编列或解码数据流。
为什么要选用它？ 使用过程中需要注意什么？ 使用过程 参考资料 Proto3 开发文档</description>
    </item>
    
  </channel>
</rss>
